{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Making Keras Model\n",
    "\n",
    "think about hyper parameters like Filter size, number of filters, which type of padding to use, which activatioon functions to use etc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "#                                                   LIBRARIES\n",
    "import os\n",
    "\n",
    "# Image preprocesssing- processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Neural Network libraries\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [],
   "source": [
    "CSV_INPUT_FOLDER = \"..\" + os.sep + \"csv\" + os.sep\n",
    "TRAIN_CSV = CSV_INPUT_FOLDER + \"train_list.csv\"\n",
    "TEST_CSV = CSV_INPUT_FOLDER + \"test_list.csv\"\n",
    "VALIDATION_CSV = CSV_INPUT_FOLDER + \"validation_list.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [112], line 65\u001B[0m\n\u001B[0;32m     61\u001B[0m     validation_data\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39marray(validation_data)\n\u001B[0;32m     63\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m train_data, train_labels, test_data, test_labels, validation_data, validation_labels\n\u001B[1;32m---> 65\u001B[0m train_data, train_labels, test_data, test_labels, validation_data, val_labels \u001B[38;5;241m=\u001B[39m \u001B[43mread_csvs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTRAIN_CSV\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTEST_CSV\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mVALIDATION_CSV\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [112], line 22\u001B[0m, in \u001B[0;36mread_csvs\u001B[1;34m(train_csv, test_csv, validation_csv)\u001B[0m\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;66;03m# convert image to array\u001B[39;00m\n\u001B[0;32m     20\u001B[0m     img \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(img)\n\u001B[1;32m---> 22\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfromarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mRGB\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m     train_data\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39marray(img))\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(train_data) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_labels)\n",
      "File \u001B[1;32m~\\.virtualenvs\\Tensorflow\\lib\\site-packages\\PIL\\Image.py:2967\u001B[0m, in \u001B[0;36mfromarray\u001B[1;34m(obj, mode)\u001B[0m\n\u001B[0;32m   2964\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ndim \u001B[38;5;241m>\u001B[39m ndmax:\n\u001B[0;32m   2965\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mToo many dimensions: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m > \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mndmax\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2967\u001B[0m size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m, shape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   2968\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m strides \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2969\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(obj, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtobytes\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "\u001B[1;31mIndexError\u001B[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "def read_csvs(train_csv = TRAIN_CSV, test_csv = TEST_CSV, validation_csv = VALIDATION_CSV):\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    validation_data = []\n",
    "\n",
    "    # convert back to numpy array\n",
    "\n",
    "    train = np.loadtxt(train_csv,dtype=str,\n",
    "                                     delimiter=',', usecols=(0, 1), unpack=True)\n",
    "\n",
    "    np.random.shuffle(train)\n",
    "\n",
    "    train_d, train_labels = train\n",
    "\n",
    "    for data in train_d:\n",
    "        data = \"..\" + data\n",
    "        img=cv2.imread(data)\n",
    "\n",
    "        # convert image to array\n",
    "        img = np.array(img)\n",
    "\n",
    "        img = Image.fromarray(img, 'RGB')\n",
    "\n",
    "        train_data.append(np.array(img))\n",
    "\n",
    "    assert len(train_data) == len(train_labels)\n",
    "\n",
    "\n",
    "    test_d, test_labels = np.loadtxt(test_csv,dtype=str,\n",
    "                                     delimiter=',', usecols=(0, 1), unpack=True)\n",
    "    for data in test_d:\n",
    "        data = \"..\" + data\n",
    "        img=cv2.imread(data)\n",
    "\n",
    "        # convert image to array\n",
    "        img = np.array(img)\n",
    "\n",
    "        img = Image.fromarray(img, 'RGB')\n",
    "\n",
    "        test_data.append(np.array(img))\n",
    "\n",
    "    validation_d, validation_labels = np.loadtxt(validation_csv,dtype=str,\n",
    "                                     delimiter=',', usecols=(0, 1), unpack=True)\n",
    "    for data in validation_d:\n",
    "        data = \"..\" + data\n",
    "        img=cv2.imread(data)\n",
    "\n",
    "        # convert image to array\n",
    "        img = np.array(img)\n",
    "\n",
    "        img = Image.fromarray(img, 'RGB')\n",
    "\n",
    "        validation_data.append(np.array(img))\n",
    "\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_labels = np.array(test_labels)\n",
    "    validation_labels = np.array(validation_labels)\n",
    "\n",
    "    train_data=np.array(train_data)\n",
    "    test_data=np.array(test_data)\n",
    "    validation_data=np.array(validation_data)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels, validation_data, validation_labels\n",
    "\n",
    "train_data, train_labels, test_data, test_labels, validation_data, val_labels = read_csvs(TRAIN_CSV, TEST_CSV, VALIDATION_CSV)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [111], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m num_classes \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m\n\u001B[1;32m----> 2\u001B[0m train_data, train_labels, test_data, test_labels, validation_data, val_labels \u001B[38;5;241m=\u001B[39m \u001B[43mread_csvs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mTRAIN_CSV\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mTEST_CSV\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mVALIDATION_CSV\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [110], line 22\u001B[0m, in \u001B[0;36mread_csvs\u001B[1;34m(train_csv, test_csv, validation_csv)\u001B[0m\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;66;03m# convert image to array\u001B[39;00m\n\u001B[0;32m     20\u001B[0m     img \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(img)\n\u001B[1;32m---> 22\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfromarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mRGB\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     24\u001B[0m     train_data\u001B[38;5;241m.\u001B[39mappend(np\u001B[38;5;241m.\u001B[39marray(img))\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(train_data) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_labels)\n",
      "File \u001B[1;32m~\\.virtualenvs\\Tensorflow\\lib\\site-packages\\PIL\\Image.py:2967\u001B[0m, in \u001B[0;36mfromarray\u001B[1;34m(obj, mode)\u001B[0m\n\u001B[0;32m   2964\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ndim \u001B[38;5;241m>\u001B[39m ndmax:\n\u001B[0;32m   2965\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mToo many dimensions: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m > \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mndmax\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 2967\u001B[0m size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m, shape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   2968\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m strides \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   2969\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(obj, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtobytes\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "\u001B[1;31mIndexError\u001B[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "num_classes = 3\n",
    "train_data, train_labels, test_data, test_labels, validation_data, val_labels = read_csvs(TRAIN_CSV, TEST_CSV, VALIDATION_CSV)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "def transform_names_into_numbers(train_labels_name, test_labels_name, validation_labels_name):\n",
    "    train_labels = test_labels = validation_labels = []\n",
    "    # train_labels\n",
    "    for idx,label in enumerate(train_labels_name):\n",
    "        if label == 'Bear':\n",
    "            train_labels.append(0)\n",
    "        elif label == 'Chicken':\n",
    "            train_labels.append(1)\n",
    "        elif label == 'Turtle':\n",
    "            train_labels.append(2)\n",
    "\n",
    "    # test_labels\n",
    "    for idx,label in enumerate(test_labels_name):\n",
    "        if label == 'Bear':\n",
    "            test_labels.append(0)\n",
    "        elif label == 'Chicken':\n",
    "            test_labels.append(1)\n",
    "        elif label == 'Turtle':\n",
    "            test_labels.append(2)\n",
    "\n",
    "    # validation_labels\n",
    "    for idx,label in enumerate(validation_labels_name):\n",
    "        if label == 'Bear':\n",
    "            validation_labels.append(0)\n",
    "        elif label == 'Chicken':\n",
    "            validation_labels.append(1)\n",
    "        elif label == 'Turtle':\n",
    "            validation_labels.append(2)\n",
    "\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_labels = np.array(test_labels)\n",
    "    validation_labels = np.array(validation_labels)\n",
    "\n",
    "    return train_labels, test_labels, validation_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "# One hot encoding for labels\n",
    "train_labels = keras.utils.to_categorical(train_labels,num_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels,num_classes)\n",
    "validation_labels = keras.utils.to_categorical(val_labels,num_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "# import sequential model and all the required layers\n",
    "def create_model():\n",
    "    #make model\n",
    "    model=Sequential()\n",
    "    # Pairs of Conv2D layer and MaxPool2D Layer with increasing filter sizes ( 16,32 ,64). This helps to make image grow more in depthwise and become more flatten.\n",
    "    # Maxpool: great as they optimize the training time\n",
    "\n",
    "    # capas de calculo -> ir de menor a mayor, suelen ser potencias / multiplos de 2 (las layers)\n",
    "\n",
    "    model.add(Conv2D(filters=64,kernel_size=3,padding=\"same\",activation=\"relu\",input_shape=(200,200,3))) # parte imp = relu\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    model.add(Conv2D(filters=128,kernel_size=3,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    model.add(Conv2D(filters=256,kernel_size=3,padding=\"same\",activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=4))\n",
    "\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten()) # without flatten output shape = (50, 50, 6) -> flatten = (None, 6) which we need to get layer output\n",
    "\n",
    "\n",
    "    # capas de clasificacion\n",
    "\n",
    "    model.add(Dense(200,activation=\"relu\"))\n",
    "\n",
    "    # Final dense layer with num_classes nodes = categories of animals we have in the set\n",
    "    # Softmax activation is used to give scores to these categories which lie between 0 and 1.\n",
    "    model.add(Dense(num_classes,activation=\"softmax\"))\n",
    "    model.summary()\n",
    "\n",
    "    # compile the model\n",
    "    # We use loss function as categorical_crossentropy and Adam optimizer\n",
    "\n",
    "    # if binary data -> loss = Binary Cross Entropy and activation = sigmoid\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_15 (Conv2D)          (None, 200, 200, 64)      1792      \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 100, 100, 64)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 100, 100, 128)     73856     \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 50, 50, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 50, 50, 256)       295168    \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 12, 12, 256)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 12, 12, 256)       0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 36864)             0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 200)               7373000   \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 603       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,744,419\n",
      "Trainable params: 7,744,419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "46/46 [==============================] - 68s 1s/step - loss: 41.8497 - accuracy: 0.3973\n",
      "Epoch 2/30\n",
      "46/46 [==============================] - 61s 1s/step - loss: 0.9158 - accuracy: 0.5747\n",
      "Epoch 3/30\n",
      "46/46 [==============================] - 58s 1s/step - loss: 0.7845 - accuracy: 0.6514\n",
      "Epoch 4/30\n",
      "46/46 [==============================] - 57s 1s/step - loss: 0.7097 - accuracy: 0.6932\n",
      "Epoch 5/30\n",
      "46/46 [==============================] - 58s 1s/step - loss: 0.5554 - accuracy: 0.7829\n",
      "Epoch 6/30\n",
      "46/46 [==============================] - 59s 1s/step - loss: 0.4781 - accuracy: 0.7973\n",
      "Epoch 7/30\n",
      "46/46 [==============================] - 59s 1s/step - loss: 0.4114 - accuracy: 0.8212\n",
      "Epoch 8/30\n",
      "46/46 [==============================] - 60s 1s/step - loss: 0.4170 - accuracy: 0.8534\n",
      "Epoch 9/30\n",
      "46/46 [==============================] - 61s 1s/step - loss: 0.3587 - accuracy: 0.8603\n",
      "Epoch 10/30\n",
      "46/46 [==============================] - 62s 1s/step - loss: 0.3101 - accuracy: 0.8760\n",
      "Epoch 11/30\n",
      "46/46 [==============================] - 57s 1s/step - loss: 0.2571 - accuracy: 0.8993\n",
      "Epoch 12/30\n",
      "46/46 [==============================] - 59s 1s/step - loss: 0.1885 - accuracy: 0.9247\n",
      "Epoch 13/30\n",
      "46/46 [==============================] - 62s 1s/step - loss: 0.1836 - accuracy: 0.9384\n",
      "Epoch 14/30\n",
      "46/46 [==============================] - 66s 1s/step - loss: 0.1563 - accuracy: 0.9473\n",
      "Epoch 15/30\n",
      "46/46 [==============================] - 63s 1s/step - loss: 0.1410 - accuracy: 0.9486\n",
      "Epoch 16/30\n",
      "46/46 [==============================] - 61s 1s/step - loss: 0.1190 - accuracy: 0.9548\n",
      "Epoch 17/30\n",
      "46/46 [==============================] - 61s 1s/step - loss: 0.2725 - accuracy: 0.9164\n",
      "Epoch 18/30\n",
      "46/46 [==============================] - 68s 1s/step - loss: 0.1707 - accuracy: 0.9384\n",
      "Epoch 19/30\n",
      "46/46 [==============================] - 63s 1s/step - loss: 0.1045 - accuracy: 0.9589\n",
      "Epoch 20/30\n",
      "46/46 [==============================] - 61s 1s/step - loss: 0.1239 - accuracy: 0.9534\n",
      "Epoch 21/30\n",
      "46/46 [==============================] - 61s 1s/step - loss: 0.1250 - accuracy: 0.9562\n",
      "Epoch 22/30\n",
      "46/46 [==============================] - 65s 1s/step - loss: 0.1765 - accuracy: 0.9473\n",
      "Epoch 23/30\n",
      "46/46 [==============================] - 58s 1s/step - loss: 0.0811 - accuracy: 0.9719\n",
      "Epoch 24/30\n",
      "46/46 [==============================] - 59s 1s/step - loss: 0.0551 - accuracy: 0.9795\n",
      "Epoch 25/30\n",
      "46/46 [==============================] - 61s 1s/step - loss: 0.1575 - accuracy: 0.9705\n",
      "Epoch 26/30\n",
      "46/46 [==============================] - 59s 1s/step - loss: 0.1048 - accuracy: 0.9678\n",
      "Epoch 27/30\n",
      "46/46 [==============================] - 58s 1s/step - loss: 0.0993 - accuracy: 0.9616\n",
      "Epoch 28/30\n",
      "46/46 [==============================] - 57s 1s/step - loss: 0.0573 - accuracy: 0.9822\n",
      "Epoch 29/30\n",
      "46/46 [==============================] - 58s 1s/step - loss: 0.0355 - accuracy: 0.9897\n",
      "Epoch 30/30\n",
      "46/46 [==============================] - 57s 1s/step - loss: 0.0299 - accuracy: 0.9897\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x18858103b20>"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "# batch potencias de 2, a mayor bacths mayor estabilidad del grandiente\n",
    "model.fit(train_data,train_labels,batch_size=32 ,epochs=15,verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 261ms/step - loss: 2.8495 - accuracy: 0.6319\n",
      "\n",
      " Test accuracy: 0.6318681240081787\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "score = model.evaluate(test_data, test_labels, verbose=1)\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# save the model\n",
    "\n",
    "'''# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")'''\n",
    "\n",
    "model.save('../models/model_200x200.h5') # Saves the entire model to a single artifact\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "def convert_to_array(img):\n",
    "    im = cv2.imread(img)\n",
    "    img = Image.fromarray(im, 'RGB')\n",
    "    return np.array(img)\n",
    "\n",
    "def get_animal_name(label):\n",
    "    if label==0:\n",
    "        return \"Bear\"\n",
    "    if label==1:\n",
    "        return \"Chicken\"\n",
    "    if label==2:\n",
    "        return \"Turtle\"\n",
    "\n",
    "def predict_animal(file):\n",
    "    print(\"Predicting .................................\")\n",
    "    # ar = convert_to_array(file)\n",
    "    file = file/255\n",
    "    a = []\n",
    "    a.append(file)\n",
    "    a = np.array(a)\n",
    "    score = model.predict(a,verbose=1)\n",
    "    label_index=np.argmax(score)\n",
    "    acc=np.max(score)\n",
    "    animal=get_animal_name(label_index)\n",
    "    print(\"The predicted Animal is a \"+animal+\" with accuracy =    \"+str(acc))\n",
    "\n",
    "    return label_index, animal"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting .................................\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "The predicted Animal is a Bear with accuracy =    0.3789469\n",
      "Predicting .................................\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "The predicted Animal is a Bear with accuracy =    0.40878782\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [106], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGood predicted: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(good_predicted))\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBad predicted: \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(bad_predicted))\n\u001B[1;32m---> 14\u001B[0m \u001B[43mvalidation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_labels\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn [106], line 6\u001B[0m, in \u001B[0;36mvalidation\u001B[1;34m(val_data, val_lab)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(val_data):\n\u001B[0;32m      5\u001B[0m     label_index, animal \u001B[38;5;241m=\u001B[39m predict_animal(data)\n\u001B[1;32m----> 6\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mlabel_index\u001B[49m \u001B[38;5;241m==\u001B[39m val_lab[idx]:\n\u001B[0;32m      7\u001B[0m         good_predicted \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "Cell \u001B[1;32mIn [106], line 6\u001B[0m, in \u001B[0;36mvalidation\u001B[1;34m(val_data, val_lab)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(val_data):\n\u001B[0;32m      5\u001B[0m     label_index, animal \u001B[38;5;241m=\u001B[39m predict_animal(data)\n\u001B[1;32m----> 6\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mlabel_index\u001B[49m \u001B[38;5;241m==\u001B[39m val_lab[idx]:\n\u001B[0;32m      7\u001B[0m         good_predicted \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1095\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1053\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.1.1\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py:169\u001B[0m, in \u001B[0;36mstop\u001B[1;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[0;32m    167\u001B[0m     frame \u001B[38;5;241m=\u001B[39m suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[1;32m--> 169\u001B[0m         \u001B[43mmain_debugger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.1.1\\plugins\\python\\helpers\\pydev\\pydevd.py:1155\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1152\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1154\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1155\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.1.1\\plugins\\python\\helpers\\pydev\\pydevd.py:1170\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1167\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1169\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1170\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1172\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1174\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def validation(val_data = validation_data, val_lab = val_labels):\n",
    "    # function not finished and not working\n",
    "    good_predicted = 0\n",
    "    bad_predicted = 0\n",
    "    for idx, data in enumerate(val_data):\n",
    "        label_index, animal = predict_animal(data)\n",
    "        if label_index == val_lab[idx]:\n",
    "            good_predicted += 1\n",
    "        else:\n",
    "            bad_predicted += 1\n",
    "\n",
    "    print(\"Good predicted: \" + str(good_predicted))\n",
    "    print(\"Bad predicted: \" + str(bad_predicted))\n",
    "\n",
    "# validation(validation_data, val_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting .................................\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "The predicted Animal is a Bear with accuracy =    0.38710973\n"
     ]
    },
    {
     "data": {
      "text/plain": "(0, 'Bear')"
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict image\n",
    "file = \"../datasets/animals/data_resized/Chicken/60.jpg\"\n",
    "file = cv2.imread(file)\n",
    "predict_animal(file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tensor board -> api to see in real time evolution of the model (from tensorflow)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}