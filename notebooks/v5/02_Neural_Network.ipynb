{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import constants\n",
    "from constants import *\n",
    "import importlib\n",
    "importlib.reload(constants)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "#from sklearn.metrics import *\n",
    "#from sklearn.metrics import classification_report\n",
    "# import seaborn as sns\n",
    "\n",
    "import sys\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate tensorflow dataset\n",
    "Si hay clases muy desbalanceadas, para balancearlas:\n",
    "\n",
    "Repetir imagenes aleatorias hasta completar las que faltan para que esté balanceado -> se retrasa el salvado de la red porque los callback no se actualizan hasta que acaba una epoch\n",
    "Elegir subconjuntos de imagenes aleatorias por cada clase del tamaño de la clase menor y darselas a la red e ir salvandola (porque la epoch ha terminado) por cada subconjunto que se le pasa"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generación del dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# get_min(dataframe)\n",
    "# given a dataframe returns the value of instances of the class with fewer instances (minimum class)\n",
    "def get_min(dataframe):\n",
    "\n",
    "    min_class = list(dataframe['class'].value_counts())[-1]\n",
    "\n",
    "    return min_class\n",
    "\n",
    "\n",
    "# create_pd_dataframe(data_csv, csv_folder = CSV_FOLDER)\n",
    "# given the name of the csv and the folder where it is located, create a pandas dataframe with its content\n",
    "# and modify the class from string to number with the class dictionary in constants\n",
    "def create_pd_dataframes(data_csv, csv_folder = CSV_FOLDER):\n",
    "    data_panda = pd.read_csv(os.path.join(csv_folder, data_csv))\n",
    "\n",
    "    # Añadir el path absoluto (la parte desde C:// ... hasta animals al inicio para poder acceder\n",
    "    data_panda[\"path\"] = [DATASETS_ROOT_FOLDER + item for item in data_panda[\"path\"]]\n",
    "\n",
    "    data_panda[\"class\"] = [CLASS_DICTIONARY[str(item)] for item in data_panda[\"class\"]]\n",
    "\n",
    "    '''\n",
    "    num_min_instances = get_min(data_panda)\n",
    "    classes = list(np.unique(data_panda['class']))\n",
    "\n",
    "    new_dataframe = pd.DataFrame([])\n",
    "\n",
    "    # new_dataframe.columns = [\"class\", \"path\"] #set the header row as the df header\n",
    "\n",
    "    for clas in classes:\n",
    "        instances_of_class = data_panda.loc[data_panda[\"class\"] == clas]\n",
    "        instances_of_class.sample(n = num_min_instances, replace = False)\n",
    "        new_dataframe = pd.concat([new_dataframe,instances_of_class])\n",
    "    '''\n",
    "    return data_panda\n",
    "\n",
    "\n",
    "\n",
    "# balance_df_classes(dataframe, mini = 0)\n",
    "# given a dataframe and the minimum name of instances we want per classs, it create a balaced dataframe\n",
    "def balance_df_classes(dataframe, mini = 0):\n",
    "    new_dataframe = pd.DataFrame([])\n",
    "\n",
    "    min_class = get_min(dataframe)\n",
    "\n",
    "    # To add a minimum number of images per class in the case of entering it by parameters\n",
    "    if mini > min_class:\n",
    "        min_class = mini\n",
    "\n",
    "    # for each class take the min number of imgs that must be taken to create the dataframe\n",
    "    for clas in list(np.unique(dataframe['class'])):\n",
    "        current_dataframe = (dataframe[dataframe['class'] == clas])\n",
    "\n",
    "        aux = current_dataframe.sample(n = min_class, replace = True)\n",
    "\n",
    "        # y lo concatena con el que ya teniamos\n",
    "        new_dataframe = pd.concat([new_dataframe,aux])\n",
    "\n",
    "    return new_dataframe\n",
    "\n",
    "# create_balanced_dataframes(data_csv, csv_folder = CSV_FOLDER, mini = 0)\n",
    "# calls the functions that create the dataframes and balance them\n",
    "def create_balanced_dataframes(data_csv, csv_folder = CSV_FOLDER, mini = 0):\n",
    "\n",
    "    pandas_dataframe = create_pd_dataframes(data_csv, csv_folder)\n",
    "    pandas_dataframe_balanced = balance_df_classes(pandas_dataframe, mini)\n",
    "\n",
    "    return pandas_dataframe_balanced"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pasar de un dataframe a un dataset de tensorflow"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# create_tf_ds(dataframe)\n",
    "# from a dataframe, it creates a tensorflow dataset\n",
    "def create_tf_ds(dataframe):\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                                  rescale=1./255.\n",
    "                                  )\n",
    "\n",
    "    tf_dataset = datagen.flow_from_dataframe(\n",
    "                            dataframe,\n",
    "                            x_col = 'path',\n",
    "                            y_col = 'class',\n",
    "                            target_size = (IMG_WIDTH,IMG_HEIGHT),\n",
    "                            color_mode = 'rgb',\n",
    "                            batch_size = 32,\n",
    "                            shuffle = True,\n",
    "                            interpolation = \"bicubic\",\n",
    "                            class_mode='raw' # categorical -> str , raw = numbers\n",
    "                            )\n",
    "\n",
    "    return tf_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modelo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "MODEL_NAME = \"test_model\"\n",
    "FROM_LOGITS = True\n",
    "MODEL_PATH = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Creates the model\n",
    "'''if MODEL_PATH:\n",
    "    model = tf.keras.models.load_model(MODEL_PATH)\n",
    "else:\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, N_CHANNELS)),\n",
    "      tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "      tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "      tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(CLASS_NUMBER)\n",
    "    ])'''\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(Conv2D(32,kernel_size=5,padding='same',activation='relu',\n",
    "        input_shape=(28,28,3)))\n",
    "model.add(MaxPool2D(padding='same'))\n",
    "model.add(Conv2D(64,kernel_size=5,padding='same',activation='relu'))\n",
    "model.add(MaxPool2D(padding='same'))\n",
    "model.add(Conv2D(128,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(MaxPool2D(padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(CLASS_NUMBER))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        2432      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        51264     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               524544    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 652,867\n",
      "Trainable params: 652,867\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hiperparametros del entramiento"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# callback to save the best model\n",
    "\n",
    "# save best model in validation\n",
    "# monitoriza el accuracy (mejor de validation)\n",
    "# solo salva el mejor a true\n",
    "# no solo salva los pesos, salva modelo + pesos en el mismo file\n",
    "# en validation se salvará el max\n",
    "# con freq cada epoch\n",
    "callback_1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "os.path.join(MODEL_SAVE_FOLDER,MODEL_NAME), monitor='val_sparse_categorical_accuracy', verbose=0, save_best_only=True,\n",
    "save_weights_only=False, mode='max', save_freq='epoch'\n",
    ")\n",
    "\n",
    "# callback to stop the model if it hasnt improved in 50 epochs\n",
    "callback_2 = tf.keras.callbacks.EarlyStopping(\n",
    "monitor='val_sparse_categorical_accuracy', min_delta=0.001, patience=50, verbose=0, mode='max',\n",
    "baseline=None, restore_best_weights=True\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Parameters (Loss, Accuracy and Optimizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# A loss object that receives the raw network output and the one-hot raw class labels is created\n",
    "# sparse porque le entran los numeros en vez del vector de onehot\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=FROM_LOGITS)\n",
    "\n",
    "# Una funcion Loss funcionaría:\n",
    "# Loss(,)\n",
    "# la red devuelve un batch con vectores = y (puede ser como logits o softmax), por otro lado están las clases reales: un batch de números representando la clase o un vector de onehot.\n",
    "# le entra un vector de la salida de la red (bacth), y el numero )\n",
    "# en un loss binario ln(0.5) = 0.69314718056 -> es aleatorio, mala interpretacion\n",
    "\n",
    "\n",
    "# a metric object that calculates the accuracy is created\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# optimizer object is created\n",
    "# it was used adam\n",
    "optimizer = SGD(learning_rate=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compile Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# model compilation\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=accuracy)\n",
    "# model.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bucle de entrenamiento"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================\n",
      "\n",
      "Generating new dataset\n",
      "Found 1257 validated image filenames.\n",
      "Found 237 validated image filenames.\n",
      "\n",
      "\n",
      "Training with created dataset\n",
      "Training with created dataset\n",
      "Training with created dataset\n",
      "\n",
      "==========================================================\n",
      "\n",
      "Generating new dataset\n",
      "Found 1257 validated image filenames.\n",
      "Found 237 validated image filenames.\n",
      "\n",
      "\n",
      "Training with created dataset\n",
      "Training with created dataset\n",
      "Training with created dataset\n",
      "\n",
      "==========================================================\n",
      "\n",
      "Generating new dataset\n",
      "Found 1257 validated image filenames.\n",
      "Found 237 validated image filenames.\n",
      "\n",
      "\n",
      "Training with created dataset\n",
      "Training with created dataset\n",
      "Training with created dataset\n"
     ]
    }
   ],
   "source": [
    "for current_epoch in range(EPOCHS):\n",
    "    # TO-DO Poner bonito el bucle para que quede clara la informacion, que no print validated image filenames\n",
    "\n",
    "\n",
    "    ######################## DATASET GENERATION ########################\n",
    "    # only when we start a new training loop dataset renewal, else we would only train with the dataset during 1 epoch\n",
    "    if current_epoch % TRAINING_LOOPS_DATASETS_RENEWAL == 0:\n",
    "        print(\"\\n==========================================================\\n\")\n",
    "        print(\"Generating new dataset\")\n",
    "        # Pandas dataframe generation from csvs and balanced\n",
    "        train_df = create_balanced_dataframes(TRAIN_CSV, CSV_FOLDER)\n",
    "        val_df = create_balanced_dataframes(VALIDATION_CSV, CSV_FOLDER, mini = 30)\n",
    "\n",
    "        # Tensorflow dataframe generation\n",
    "        train_tf_dataset = create_tf_ds(train_df)\n",
    "        validation_tf_dataset = create_tf_ds(val_df)\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "    ######################## TRAINING PHASE ########################\n",
    "    print(\"Training with created dataset\")\n",
    "    # h = model.fit(train_tf_dataset, epochs=1, validation_data = validation_tf_dataset, batch_size = BATCH_NUMBER, callbacks= [callback_1, callback_2], verbose = 1)\n",
    "    # print(\"\\n==========================================================\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# In case it is decided to train more than 100 epochs\n",
    "for i in range(epochs):\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "    #SECUENCIA PARA CONFECCIÓN DEL CSV\n",
    "\n",
    "    history['epoch'] = list(range(0,len(history.index)))\n",
    "\n",
    "    history.to_csv(os.path.join(CSV_FOLDER, name_csv_history), header=True, index=False)\n",
    "\n",
    "    # Plot history: CE\n",
    "    plt.figure()\n",
    "    plt.plot(history['epoch'], history['loss'], label='Loss (training data)')\n",
    "    plt.plot(history['epoch'], history['val_loss'], label='Loss (validation data)')\n",
    "    plt.title('Loss')\n",
    "    plt.ylabel('CE')\n",
    "    plt.xlabel('Nº epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot history: Accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(history['epoch'], history['categorical_accuracy'], label='Accuracy (training data)')\n",
    "    plt.plot(history['epoch'], history['val_categorical_accuracy'], label='Accuracy (validation data)')\n",
    "    plt.title('Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Nº epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    # show training time\n",
    "    fin = time.time()\n",
    "    print(round((fin - inicio)/60, 0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# In case it is decided to train more than 100 epochs\n",
    "for i in range(epochs):\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "    #SECUENCIA PARA CONFECCIÓN DEL CSV\n",
    "\n",
    "    history['epoch'] = list(range(0,len(history.index)))\n",
    "\n",
    "    history.to_csv(os.path.join(CSV_FOLDER, name_csv_history), header=True, index=False)\n",
    "\n",
    "    # Plot history: CE\n",
    "    plt.figure()\n",
    "    plt.plot(history['epoch'], history['loss'], label='Loss (training data)')\n",
    "    plt.plot(history['epoch'], history['val_loss'], label='Loss (validation data)')\n",
    "    plt.title('Loss')\n",
    "    plt.ylabel('CE')\n",
    "    plt.xlabel('Nº epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot history: Accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(history['epoch'], history['categorical_accuracy'], label='Accuracy (training data)')\n",
    "    plt.plot(history['epoch'], history['val_categorical_accuracy'], label='Accuracy (validation data)')\n",
    "    plt.title('Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Nº epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    # show training time\n",
    "    fin = time.time()\n",
    "    print(round((fin - inicio)/60, 0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}