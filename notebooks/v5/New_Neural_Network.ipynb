{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    " # Neural Network\n",
    " Tensorflow dataframe creation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 07:51:01.328364: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-21 07:51:01.459049: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-21 07:51:02.054699: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib64/libcudnn.so.8:/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/extras/CUPTI/lib64\n",
      "2022-10-21 07:51:02.054802: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/lib64/libcudnn.so.8:/usr/local/cuda-11.2/lib64:/usr/local/cuda-11.2/extras/CUPTI/lib64\n",
      "2022-10-21 07:51:02.054811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import constants\n",
    "from constants import *\n",
    "import importlib\n",
    "importlib.reload(constants)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "#from sklearn.metrics import *\n",
    "#from sklearn.metrics import classification_report\n",
    "# import seaborn as sns\n",
    "\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generate tensorflow dataset\n",
    "\n",
    "Si hay clases muy desbalanceadas, para balancearlas:\n",
    "1. *Repetir imagenes aleatorias* hasta completar las que faltan para que esté balanceado -> se retrasa el salvado de la red porque los callback no se actualizan hasta que acaba una epoch\n",
    "2. *Elegir subconjuntos de imagenes aleatorias por cada clase del tamaño de la clase menor y darselas a la red* e ir salvandola (porque la epoch ha terminado) por cada subconjunto que se le pasa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generación del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_min(dataframe):    \n",
    "    \n",
    "    min_class = list(dataframe['class'].value_counts())[-1]\n",
    "    \n",
    "    return min_class\n",
    "\n",
    "\n",
    "def create_pd_dataframes(data_csv, csv_folder = CSV_FOLDER):\n",
    "    data_panda = pd.read_csv(os.path.join(csv_folder, data_csv))\n",
    "\n",
    "    # Añadir el path absoluto (la parte desde C:// ... hasta animals al inicio para poder acceder\n",
    "    data_panda[\"path\"] = [DATASETS_ROOT_FOLDER + item for item in data_panda[\"path\"]]\n",
    "    \n",
    "    data_panda[\"class\"] = [CLASS_DICTIONARY[str(item)] for item in data_panda[\"class\"]]\n",
    "    \n",
    "    '''\n",
    "    num_min_instances = get_min(data_panda)\n",
    "    classes = list(np.unique(data_panda['class']))\n",
    "\n",
    "    new_dataframe = pd.DataFrame([])\n",
    "\n",
    "    # new_dataframe.columns = [\"class\", \"path\"] #set the header row as the df header\n",
    "\n",
    "    for clas in classes:\n",
    "        instances_of_class = data_panda.loc[data_panda[\"class\"] == clas]\n",
    "        instances_of_class.sample(n = num_min_instances, replace = False)\n",
    "        new_dataframe = pd.concat([new_dataframe,instances_of_class])\n",
    "    '''\n",
    "    return data_panda\n",
    "\n",
    "\n",
    "\n",
    "def balance_df_classes(dataframe, mini = 0):\n",
    "    new_dataframe = pd.DataFrame([])\n",
    "\n",
    "    min_class = get_min(dataframe)\n",
    "\n",
    "    # Para añadir un numero minimo de imagenes por clase en el caso de meterla por parametros\n",
    "    if mini > min_class:\n",
    "        min_class = mini\n",
    "\n",
    "    # para cada clase coje el numero min de imgs que hay q coger para crear el dataframe\n",
    "    for clas in list(np.unique(dataframe['class'])):\n",
    "        current_dataframe = (dataframe[dataframe['class'] == clas])\n",
    "\n",
    "        aux = current_dataframe.sample(n = min_class, replace = True)\n",
    "\n",
    "        # y lo concatena con el que ya teniamos\n",
    "        new_dataframe = pd.concat([new_dataframe,aux])\n",
    "\n",
    "    return new_dataframe\n",
    "\n",
    "\n",
    "def create_balanced_dataframes(data_csv, csv_folder = CSV_FOLDER, mini = 0):\n",
    "    \n",
    "    pandas_dataframe = create_pd_dataframes(data_csv, csv_folder)\n",
    "    pandas_dataframe_balanced = balance_df_classes(pandas_dataframe, mini)\n",
    "    \n",
    "    return pandas_dataframe_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pasar de un dataframe a un dataset de tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_tf_ds(dataframe):\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                                  rescale=1./255.\n",
    "                                  )\n",
    "\n",
    "    tf_dataset = datagen.flow_from_dataframe(\n",
    "                            dataframe,\n",
    "                            x_col = 'path',\n",
    "                            y_col = 'class',\n",
    "                            target_size = (IMG_WIDTH,IMG_HEIGHT),\n",
    "                            color_mode = 'grayscale',\n",
    "                            batch_size = 32,\n",
    "                            shuffle = True,\n",
    "                            interpolation = \"bicubic\",\n",
    "                            class_mode='raw' # categorical -> str , raw = numbers\n",
    "                            )\n",
    "\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"test_model\"\n",
    "FROM_LOGITS = True\n",
    "MODEL_PATH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-21 07:51:02.704564: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-10-21 07:51:02.704598: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: despachoPC\n",
      "2022-10-21 07:51:02.704605: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: despachoPC\n",
      "2022-10-21 07:51:02.704727: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.141.3\n",
      "2022-10-21 07:51:02.704750: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.141.3\n",
      "2022-10-21 07:51:02.704756: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.141.3\n",
      "2022-10-21 07:51:02.705043: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "'''if MODEL_PATH:\n",
    "    model = tf.keras.models.load_model(MODEL_PATH)\n",
    "else:\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, N_CHANNELS)),\n",
    "      tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "      tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "      tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(CLASS_NUMBER)\n",
    "    ])'''\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(Conv2D(32,kernel_size=5,padding='same',activation='relu',\n",
    "        input_shape=(28,28,1)))\n",
    "model.add(MaxPool2D(padding='same'))\n",
    "model.add(Conv2D(64,kernel_size=5,padding='same',activation='relu'))\n",
    "model.add(MaxPool2D(padding='same'))\n",
    "model.add(Conv2D(128,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(MaxPool2D(padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(CLASS_NUMBER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 32)        832       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        51264     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               524544    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 653,066\n",
      "Trainable params: 653,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hiperparametros del entramiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# callback to save the best model\n",
    "\n",
    "# save best model in validation\n",
    "# monitoriza el accuracy (mejor de validation)\n",
    "# solo salva el mejor a true\n",
    "# no solo salva los pesos, salva modelo + pesos en el mismo file\n",
    "# en validation se salvará el max\n",
    "# con freq cada epoch\n",
    "callback_1 = tf.keras.callbacks.ModelCheckpoint(\n",
    "os.path.join(MODEL_SAVE_FOLDER,MODEL_NAME), monitor='val_sparse_categorical_accuracy', verbose=0, save_best_only=True,\n",
    "save_weights_only=False, mode='max', save_freq='epoch'\n",
    ")\n",
    "\n",
    "# callback to stop the model if it hasnt improved in 50 epochs\n",
    "callback_2 = tf.keras.callbacks.EarlyStopping(\n",
    "monitor='val_sparse_categorical_accuracy', min_delta=0.001, patience=50, verbose=0, mode='max',\n",
    "baseline=None, restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define Parameters (Loss, Accuracy and Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# A loss object that receives the raw network output and the one-hot raw class labels is created\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=FROM_LOGITS)\n",
    "\n",
    "# a metric object that calculates the accuracy is created\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "# optimizer object is created\n",
    "optimizer = SGD(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# compilación del modelo\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=accuracy)\n",
    "# model.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Found 30350 validated image filenames.\n",
      "Found 5700 validated image filenames.\n",
      "Epoch 1/3\n",
      "946/949 [============================>.] - ETA: 0s - loss: 0.8751 - sparse_categorical_accuracy: 0.7424"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /media/manuel/Extreme SSD/00 IMPORTANTE/Dropbox_Total/Dropbox_Almacen/Estudios/20221009 Alba/Animal_Recognition_Neural_Network-master/models/v5/test_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /media/manuel/Extreme SSD/00 IMPORTANTE/Dropbox_Total/Dropbox_Almacen/Estudios/20221009 Alba/Animal_Recognition_Neural_Network-master/models/v5/test_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "949/949 [==============================] - 22s 23ms/step - loss: 0.8731 - sparse_categorical_accuracy: 0.7430 - val_loss: 0.2631 - val_sparse_categorical_accuracy: 0.9240\n",
      "Epoch 2/3\n",
      "947/949 [============================>.] - ETA: 0s - loss: 0.1690 - sparse_categorical_accuracy: 0.9484"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /media/manuel/Extreme SSD/00 IMPORTANTE/Dropbox_Total/Dropbox_Almacen/Estudios/20221009 Alba/Animal_Recognition_Neural_Network-master/models/v5/test_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /media/manuel/Extreme SSD/00 IMPORTANTE/Dropbox_Total/Dropbox_Almacen/Estudios/20221009 Alba/Animal_Recognition_Neural_Network-master/models/v5/test_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "949/949 [==============================] - 18s 19ms/step - loss: 0.1690 - sparse_categorical_accuracy: 0.9485 - val_loss: 0.1360 - val_sparse_categorical_accuracy: 0.9579\n",
      "Epoch 3/3\n",
      "949/949 [==============================] - 17s 18ms/step - loss: 0.1056 - sparse_categorical_accuracy: 0.9674 - val_loss: 0.1501 - val_sparse_categorical_accuracy: 0.9540\n",
      "\n",
      "\n",
      "\n",
      "Found 30350 validated image filenames.\n",
      "Found 5700 validated image filenames.\n",
      "Epoch 1/3\n",
      "948/949 [============================>.] - ETA: 0s - loss: 0.0907 - sparse_categorical_accuracy: 0.9722"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /media/manuel/Extreme SSD/00 IMPORTANTE/Dropbox_Total/Dropbox_Almacen/Estudios/20221009 Alba/Animal_Recognition_Neural_Network-master/models/v5/test_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /media/manuel/Extreme SSD/00 IMPORTANTE/Dropbox_Total/Dropbox_Almacen/Estudios/20221009 Alba/Animal_Recognition_Neural_Network-master/models/v5/test_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "949/949 [==============================] - 19s 20ms/step - loss: 0.0906 - sparse_categorical_accuracy: 0.9722 - val_loss: 0.0866 - val_sparse_categorical_accuracy: 0.9763\n",
      "Epoch 2/3\n",
      "421/949 [============>.................] - ETA: 8s - loss: 0.0707 - sparse_categorical_accuracy: 0.9784"
     ]
    }
   ],
   "source": [
    "for i in range(TRAINING_LOOPS_DATASETS_RENEWAL):\n",
    "    # TO-DO Poner bonito el bucle para que quede clara la informacion, que no print validated image filenames\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    ############## DATASET GENERATION ########################\n",
    "    \n",
    "    # Pandas dataframe generation from csvs and balanced\n",
    "    train_df = create_balanced_dataframes(TRAIN_CSV, CSV_FOLDER)   \n",
    "    val_df = create_balanced_dataframes(VALIDATION_CSV, CSV_FOLDER, mini = 30)\n",
    "   \n",
    "    # Tensorflow dataframe generation\n",
    "    train_tf_dataset = create_tf_ds(train_df)\n",
    "    validation_tf_dataset = create_tf_ds(val_df)\n",
    "    \n",
    "    \n",
    "    ############### TRAINING PHASE ###################\n",
    "    \n",
    "    h = model.fit(train_tf_dataset, epochs= EPOCHS, validation_data = validation_tf_dataset, batch_size = BATCH_NUMBER, callbacks= [callback_1, callback_2], verbose = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#   with tf.device('/GPU:0'):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# In case it is decided to train more than 100 epochs\n",
    "for i in range(epochs):\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "    #SECUENCIA PARA CONFECCIÓN DEL CSV\n",
    "\n",
    "    history['epoch'] = list(range(0,len(history.index)))\n",
    "\n",
    "    history.to_csv(os.path.join(CSV_FOLDER, name_csv_history), header=True, index=False)\n",
    "\n",
    "    # Plot history: CE\n",
    "    plt.figure()\n",
    "    plt.plot(history['epoch'], history['loss'], label='Loss (training data)')\n",
    "    plt.plot(history['epoch'], history['val_loss'], label='Loss (validation data)')\n",
    "    plt.title('Loss')\n",
    "    plt.ylabel('CE')\n",
    "    plt.xlabel('Nº epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot history: Accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(history['epoch'], history['categorical_accuracy'], label='Accuracy (training data)')\n",
    "    plt.plot(history['epoch'], history['val_categorical_accuracy'], label='Accuracy (validation data)')\n",
    "    plt.title('Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Nº epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    # show training time\n",
    "    fin = time.time()\n",
    "    print(round((fin - inicio)/60, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# In case it is decided to train more than 100 epochs\n",
    "for i in range(epochs):\n",
    "\n",
    "    with tf.device('/device:GPU:0'):\n",
    "    #SECUENCIA PARA CONFECCIÓN DEL CSV\n",
    "\n",
    "    history['epoch'] = list(range(0,len(history.index)))\n",
    "\n",
    "    history.to_csv(os.path.join(CSV_FOLDER, name_csv_history), header=True, index=False)\n",
    "\n",
    "    # Plot history: CE\n",
    "    plt.figure()\n",
    "    plt.plot(history['epoch'], history['loss'], label='Loss (training data)')\n",
    "    plt.plot(history['epoch'], history['val_loss'], label='Loss (validation data)')\n",
    "    plt.title('Loss')\n",
    "    plt.ylabel('CE')\n",
    "    plt.xlabel('Nº epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot history: Accuracy\n",
    "    plt.figure()\n",
    "    plt.plot(history['epoch'], history['categorical_accuracy'], label='Accuracy (training data)')\n",
    "    plt.plot(history['epoch'], history['val_categorical_accuracy'], label='Accuracy (validation data)')\n",
    "    plt.title('Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Nº epoch')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "    # show training time\n",
    "    fin = time.time()\n",
    "    print(round((fin - inicio)/60, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}